{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with ***Super Mario Bros.***: Baseline Model\n",
    "\n",
    "## Project Members\n",
    "\n",
    "* Sam Kolins\n",
    "* Atishay Sehgal\n",
    "* Arpita Shah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install keras\n",
    "!pip install gym\n",
    "!pip install gym_super_mario_bros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from nes_py.wrappers import BinarySpaceToDiscreteSpaceEnv\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Agent Class: We create the agent that interacts with the a state of the environment at each step by performing an action for which he is rewarded. We use the Huber Loss and the Adam optimizer to maximize the reward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of episodes to run\n",
    "EPISODES = 100\n",
    "\n",
    "\n",
    "#Creating the agent class\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.99    # discount rate\n",
    "        self.epsilon = 0.95  # exploration rate\n",
    "        self.epsilon_min = 0.92\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.learning_rate = 0.05\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "        #Defining a loss function       \n",
    "    def _huber_loss(self, target, prediction):\n",
    "        # sqrt(1+error^2)-1\n",
    "        error = prediction - target\n",
    "        return K.mean(K.sqrt(1+K.square(error))-1, axis=-1)\n",
    "\n",
    "    # The Architecture\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(48, activation='relu'))\n",
    "        model.add(Dense(72, activation='relu'))\n",
    "        model.add(Dense(96, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss=self._huber_loss,\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    # updating the target model by adding new weights\n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    # create a memory to learn\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    # take steps based on probabilities (choose between exploration and exploitation)\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "    # accrue reward based on recursive replay\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] += reward\n",
    "            else:\n",
    "                # a = self.model.predict(next_state)[0]\n",
    "                t = self.target_model.predict(next_state)[0]\n",
    "                target[0][action] += reward + self.gamma * np.amax(t)\n",
    "                #target[0][action] = reward + self.gamma * t[np.argmax(t)]\n",
    "            #saving the graph to tensorboard\n",
    "            #!mkdir Graph\n",
    "            #tbcall = TensorBoard(log_dir = './tmp/hw5_q2_log/Graph',write_graph = True,write_images = True)\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)#, callbacks = [tbcall])\n",
    "            #!tensorboard --logdir ./tmp/hw5_q2_log/Graph\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally watch as mario attempts to learn how to reach the flag at the end of the level in a down-sampled version of the game. We choose the ***simple movement*** list out of three choices provided by gym_super_mario_bros. At each step, we save the rewards. At the end of 5 episodes we see the average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe for all the saved rewards at each step\n",
    "data ={'episodes': [],'step': [], 'epsilon': [], 'reward': []}\n",
    "avrev=[]\n",
    "finavrev=[]\n",
    "\n",
    "#rendering the enviroment and playing out the episodes\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"SuperMarioBros-v2\")\n",
    "    env = BinarySpaceToDiscreteSpaceEnv(env, SIMPLE_MOVEMENT)\n",
    "    state_size = env.observation_space.shape[1]\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    done = False\n",
    "    batch_size = 50000\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        state = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)\n",
    "        #state = np.reshape(state, [1, state_size])\n",
    "        for time in range(500):\n",
    "            env.render()\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = cv2.cvtColor(next_state, cv2.COLOR_BGR2GRAY)\n",
    "            reward += reward if not done else -50 #state == env.reset()\n",
    "            #next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            agent.update_target_model()\n",
    "            #print(\"episode: {}/{}, score: {}, e: {:.2}, r: {}, a: {}\".format(e, EPISODES, time, agent.epsilon, reward, action))\n",
    "            data['episodes'].append(e)\n",
    "            data['step'].append(time)\n",
    "            data['epsilon'].append(agent.epsilon)\n",
    "            data['reward'].append(reward)\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "            if (e+1) % 5 == 0:\n",
    "                #avrev.append(np.mean(reward))\n",
    "                print('After {} episodes, last 5 rewards averaged {}'.format(e+1, np.mean(avrev[-5:])))\n",
    "                avrev.append(np.mean(reward))\n",
    "        plt.plot(avrev, linewidth=2)\n",
    "        plt.xlabel('episode')\n",
    "        plt.ylabel('average reward per episode')\n",
    "        plt.title('DQN SuperMarioBot')\n",
    "        plt.show()\n",
    "        #save the reward plots\n",
    "        plt.savefig(\"basic.png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
